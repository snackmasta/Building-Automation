\
import unittest
import os
import sys
import json
from datetime import datetime

# Add project root to sys.path to allow importing modules from src and tests
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Corrected imports: Assuming unit_tests and system_validator are in the same 'tests' directory as test_runner.py
import unit_tests
import system_validator

def run_all_tests():
    """Runs all unit tests and system validations, then generates a combined report."""
    print("======================================================================")
    print("Wastewater Treatment Plant - Comprehensive Test Suite")
    print("======================================================================")
    start_time = datetime.now()

    # --- Run Unit Tests ---
    print("\\nRunning Unit Tests...")
    try:
        unit_test_results = unit_tests.run_tests() # Corrected: Call the function directly
        unit_tests_passed = unit_test_results.testsRun - len(unit_test_results.failures) - len(unit_test_results.errors)
        unit_tests_failed = len(unit_test_results.failures) + len(unit_test_results.errors)
        unit_tests_total = unit_test_results.testsRun
        print(f"Unit Tests Completed: {unit_tests_passed}/{unit_tests_total} passed.")
    except Exception as e:
        print(f"Error running unit tests: {e}")
        unit_test_results = None
        unit_tests_passed = 0
        unit_tests_failed = 1 # Mark as failed
        unit_tests_total = 1

    # --- Run System Validation ---
    print("\\nRunning System Validation...")
    system_validator_instance = system_validator.WWTPSystemValidator() # Corrected: Instantiate the class
    try:
        system_validation_results = system_validator_instance.run_full_validation() # Corrected: Call the method on the instance
        system_validation_status = system_validation_results.get('overall_status', 'error')
        system_validation_score = system_validation_results.get('summary', {}).get('overall_score', 0)
        print(f"System Validation Completed: Status - {system_validation_status}, Score - {system_validation_score:.2f}%")
    except Exception as e:
        print(f"Error running system validation: {e}")
        system_validation_results = {"overall_status": "error", "summary": {"overall_score": 0}, "error_message": str(e)}
        system_validation_status = "error"

    # --- Combine Results ---
    combined_report = {
        "timestamp": start_time.isoformat(),
        "overall_status": "pending",
        "summary": {
            "total_tests": 0,
            "total_passed": 0,
            "total_failed": 0,
        },
        "unit_tests": {},
        "system_validation": {},
    }

    if unit_test_results:
        combined_report["unit_tests"] = {
            "status": "passed" if unit_tests_failed == 0 else "failed",
            "total": unit_tests_total,
            "passed": unit_tests_passed,
            "failed": unit_tests_failed,
            "skipped": len(unit_test_results.skipped) if hasattr(unit_test_results, 'skipped') else 0,
            # Storing the path to the detailed JSON report generated by unit_tests.py
            "details_report_path": os.path.join(project_root, 'tests', 'test_results.json')
        }

    combined_report["system_validation"] = {
        "status": system_validation_status,
        "overall_score": system_validation_results.get('summary', {}).get('overall_score', 0),
        "categories": system_validation_results.get('test_categories', {}),
        "details_report_path": os.path.join(project_root, 'reports', f"system_validation_report_{start_time.strftime('%Y%m%d_%H%M%S')}.json") # Path for new report
    }
    
    # Save system validation detailed report
    try:
        os.makedirs(os.path.join(project_root, 'reports'), exist_ok=True)
        with open(combined_report["system_validation"]["details_report_path"], 'w') as f_sys:
            json.dump(system_validation_results, f_sys, indent=4)
        print(f"System validation detailed report saved to: {combined_report['system_validation']['details_report_path']}")
    except Exception as e:
        print(f"Error saving system validation report: {e}")


    # Calculate overall status
    total_passed = unit_tests_passed
    total_tests = unit_tests_total
    
    # Consider system validation pass/fail based on a threshold, e.g., 90% score
    system_validation_passed = system_validation_status.lower() == 'passed' or \
                               system_validation_status.lower() == 'excellent' or \
                               (system_validation_status.lower() == 'warning' and system_validation_score >= 70)

    if unit_tests_failed == 0 and system_validation_passed:
        combined_report["overall_status"] = "passed"
    else:
        combined_report["overall_status"] = "failed"

    combined_report["summary"]["total_passed"] = unit_tests_passed + (1 if system_validation_passed else 0)
    combined_report["summary"]["total_tests"] = unit_tests_total + 1 # System validation is one major test
    combined_report["summary"]["total_failed"] = (unit_tests_total - unit_tests_passed) + (0 if system_validation_passed else 1)


    # --- Save Combined Report ---
    report_dir = os.path.join(project_root, "reports")
    os.makedirs(report_dir, exist_ok=True)
    combined_report_path = os.path.join(report_dir, f"comprehensive_test_report_{start_time.strftime('%Y%m%d_%H%M%S')}.json")
    
    try:
        with open(combined_report_path, 'w') as f:
            json.dump(combined_report, f, indent=4)
        print(f"\\nComprehensive test report saved to: {combined_report_path}")
    except Exception as e:
        print(f"Error saving combined report: {e}")

    end_time = datetime.now()
    print(f"Total execution time: {end_time - start_time}")
    print("======================================================================")

    if combined_report["overall_status"] != "passed":
        sys.exit(1) # Exit with error code if tests failed

if __name__ == "__main__":
    run_all_tests()
